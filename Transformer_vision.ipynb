{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27060\\4257356806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMultiHeadedSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Multi-headed self-attention layer\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange \n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, indim, adim, nheads, drop):\n",
    "        '''\n",
    "        indim : (int) input dimension of input vector\n",
    "        adim: (int) dimension of each attention head\n",
    "        nheads: (int) number of heads in the multi-headed attention layer\n",
    "        drop: (float 0~1) probabilty of dropping a node\n",
    "        \n",
    "        Implements QKV multi-headed attention layer\n",
    "        output = softmax(Q*K/sqrt(d))*V\n",
    "        scale = 1/sqrt(d), here, d=adim\n",
    "        '''\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "        hdim = adim*nheads\n",
    "        self.scale= hdim**-0.5 # scale in softmax(Q*K*scale)*V\n",
    "        # Create a list of nheads (key, value, query) layers.\n",
    "        # Each layer has 3 linear layers, one for each qkv.\n",
    "\n",
    "        #nn.Linear(indim, hdim, bias=False)\n",
    "        #there are nheads number of linear layers, each with hdim output\n",
    "        self.query_lyr = self.get_qkv_layer(indim, hdim, nheads)\n",
    "        self.value_lyr = self.get_qkv_layer(indim, hdim, nheads)\n",
    "        \n",
    "        self.attention_scores = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        \n",
    "        self.out_layer = nn.Sequential(Rearrange('bsize nheads indim hdim -> bsize (nheads indim) hdim'),\n",
    "                                       nn.Linear(hdim, indim,),\n",
    "                                       nn.Dropout(drop))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.key_lyr(x)\n",
    "        key = self.query_lyr(x)\n",
    "        value = self.value_lyr(x)\n",
    "        \n",
    "        dotp = torch.matmul(query, key.transpose(-1,-2))*self.scale\n",
    "        \n",
    "        scores = self.attention_scores(dotp)\n",
    "        \n",
    "        scores = self.dropout(scores)\n",
    "        \n",
    "        weighted = torch.matmul(scores, value)\n",
    "        \n",
    "        out = self.out_layer(weighted)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transformer Encoder Layer'''\n",
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    Although torch has a nn.Transformer class, it includes both encoder and decoder layers (with cross attention). Since ViT requires only the encoder, we can't use nn.Transformer.\n",
    "    So we implement our own Transformer encoder layer class\n",
    "    '''\n",
    "    def __init__(self, nheads, nlayers, embed_dim, head_dim, mlp_hdim, dropout):\n",
    "        '''\n",
    "        nheads: (int) number of heads in the multi-headed attention layer\n",
    "        nlayers: (int) number of multi-headed attention layers in the encoder\n",
    "        embed_dim: (int) input dimension of input tokens\n",
    "        head_dim: (int) dimension of each attention head\n",
    "        mlp_hdim: (int) number of hidden dimensions in hidden layer of MLP\n",
    "        dropoutL: (float 0~1) probabilty of dropping a node\n",
    "        '''\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.nlayers = nlayers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.mlp_hdim = mlp_hdim\n",
    "        self.drop_prob = dropout\n",
    "        \n",
    "        self.salayers, self.fflayers = self.getlayers()\n",
    "        \n",
    "    def getlayers(self):\n",
    "        samodules = nn.ModuleList()\n",
    "        ffmodules = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.nlayers):\n",
    "            sam = nn.Sequential(\n",
    "                nn.LayerNorm(self.embed_dim),\n",
    "                MultiHeadedSelfAttention(\n",
    "                    self.embed_dim,\n",
    "                    self.head_dim,\n",
    "                    self.nheads,\n",
    "                    self.drop_prob\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            samodules.append(sam)\n",
    "            \n",
    "            ffm = nn.Sequential(\n",
    "                nn.LayerNorm(self.embed_dim),\n",
    "                nn.Linear(self.embed_dim, self.mlp_hdim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(self.drop_prob),\n",
    "                nn.Linear(self.mlp_hdim, self.embed_dim),\n",
    "                nn.Dropout(self.drop_prob)\n",
    "            )\n",
    "            \n",
    "            ffmodules.append(ffm)\n",
    "        \n",
    "        return samodulesm, ffmodules\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for (sal,ffl) in zip(self.salayers, self.fflayers):\n",
    "            x = x + sal(x)\n",
    "            x = x + ffl(x)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vision Transformer Class\n",
    "'''\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        input_size = cfg['input_size']\n",
    "        self.patch_size = cfg['patch_size']\n",
    "        self.embed_dim = cfg['embed_dim']\n",
    "        salayers = cfg['salayers']\n",
    "        nheads = cfg['nheads']\n",
    "        head_dim = cfg['head_dim']\n",
    "        mlp_hdim = cfg['mlp_hdim']\n",
    "        drop_prob = cfg['drop_prob']\n",
    "        nclasses = cfg['nclasses']\n",
    "        \n",
    "        self.num_patches = (input_size[0]//self.patch_size[0])*(input_size[1]//self.patch_size) + 1\n",
    "        \n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h px) (w py) -> b (h w) (px py c)', px=self.patch_size[0], py=self.patch_size),\n",
    "            nn.Linear(self.patch_size*self.patch_size[1]*input_size[2], self.embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1, self.embed_dim))\n",
    "        '''Similar to BERT, we add a cls token as a learnable parameter at the beginning of the ViT model. This token is evoked with self attention\n",
    "        and is used to predict the class of the image at the end. Tokens from all patches are IGNORED.\n",
    "        '''\n",
    "        self.positional_embedding = nn.Parameter(torch.rand(1, self.num_patches+1, self.embed_dim))\n",
    "        #learnable positional embedding\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            nheads = nheads,\n",
    "            nlayers = salayers,\n",
    "            embed_dim = self.embed_dim,\n",
    "            head_dim = head_dim,\n",
    "            mlp_hdim = mlp_hdim,\n",
    "            dropout = drop_prob\n",
    "        )\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(nn.LayerNorm(self.embed_dim), nn.Linear(self.embed_dim, nclasses)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the forward pass\n",
    "\n",
    "def forward(self, x):\n",
    "    #x is in the format of (batch_size, channels, height, width)\n",
    "    npatches  = (x.size(2)//self.patch_size[0])*(x.size(3)//self.patch_size) + 1\n",
    "    embed = self.patch_embedding(x)\n",
    "    \n",
    "    x = torch.cat((self.cls_token.expand(x.size(0), -1, -1), embed), dim=1)\n",
    "    #repeat class token for every sample in batch and cat along patch dimension, so class is treated as a patch\n",
    "    \n",
    "    if npatches == self.num_patches:\n",
    "        x = x + self.positional_embedding\n",
    "        #if the image is the same size as the training images, add the positional embedding\n",
    "    else:\n",
    "        interpolated = nn.functional.interpolate(\n",
    "            self.positional_embedding[None], #add a dummy dimension\n",
    "            (npatches+1, self.embed_dim),\n",
    "            mode='bicubic'\n",
    "        )\n",
    "        #we use bilinear but only linear will be used for the cls token\n",
    "        x+= interpolated[0]#remove dummy dimension\n",
    "    \n",
    "    x = self.dropout_layer(x)\n",
    "    \n",
    "    x = self.transformer(x)\n",
    "    \n",
    "    x = x[:,0,:] #use the first token (cls token) to predict the class and ignore the rest\n",
    "    \n",
    "    pred = self.prediction_head(x)\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install darklight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darklight as dl\n",
    "import torch\n",
    "from vit import VisionTransformer\n",
    "import vitconfigs as vcfg\n",
    " \n",
    "net=VisionTransformer(vcfg.base)\n",
    "dm=dl.ImageNetManager('/sfnvme/imagenet/', size=[224,224], bsize=128)\n",
    " \n",
    "opt_params={\n",
    "  'optimizer': torch.optim.AdamW,\n",
    "  'okwargs': {'lr': 1e-4, 'weight_decay':0.05},\n",
    "  'scheduler':torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "  'skwargs': {'T_0':10,'T_mult':2},\n",
    "  'amplevel': None\n",
    "  }\n",
    "trainer=dl.StudentTrainer(net, dm, None, opt_params=opt_params)\n",
    "trainer.train(epochs=300, save='vitbase_{}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "151f772aca39fdeb15870b9c3daead7347fbf11174488484ca3664dd2df728b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
